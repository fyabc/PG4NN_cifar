{
    /// How to set configures in command line
    /// When set global configures:
    ///  G.key=exp
    /// When set policy configures:
    ///  P.key=exp
    /// When set configures in current model ("cifar10" in CIFAR-10, "imdb" in IMDB):
    ///  key=exp
    ///
    /// key is the name of the parameter, exp is the value.
    /// [NOTE]: exp must be a legal Python expression, the value of the key is eval(exp).
    /// [NOTE]: in exp, use @ to represent double quotes " (used in string values)
    /// [NOTE]: you must NOT have any spaces in the expression above.

    /// Basic configurations.
//    "dataset": "CIFAR10",
    "seed": 1234,
    "floatX": "float32",
    "logging_file": "./data/log.txt",

    /// Train types.
    /// 'raw':           train ResNet directly.
    /// 'policy':        train policy.
    /// 'deterministic': run deterministic policy (alpha loss)
    /// 'stochastic':    run stochastic policy (randomly drop data)
    "train_type": "policy",

    /// TODO: Configuration to be moved into "cifar10"
    "model": null,

    /// Set this flag to some string to run some temp jobs.
    // Temp job code must be in this flag, such as:
    //     if Config['temp_job'] == 'xxx':
    //         ......
    "temp_job": "",

    "policy": {
        /// Name of policy network
        // Candidates:
        //     LRPolicyNetwork
        //     MLPPolicyNetwork
        "policy_model_name": "LRPolicyNetwork",

        "policy_model_file": "./data/policy_model.npz",
        "policy_save_freq": 1,

        "policy_warm_start": false,
        "policy_warm_start_file": null,

        "num_episodes": 1000,

        /// Policy input features
        "add_label_input": true,
        "add_label": false,
        "use_first_layer_output": false,
        "add_margin": true,
        "add_average_accuracy": true,
        "add_epoch_number": false,
        "add_learning_rate": false,
        "add_loss_rank": true,
        "add_accepted_data_number": true,

        /// Immediate reward & speed reward
        "immediate_reward": false,
        "immediate_reward_sample_size": 10000,

        "speed_reward": false,
        "speed_reward_threshold": 0.8,

        "speed_reward_config": [
            [0.95, 0.1666667],
            [0.97, 0.3333333],
            [0.98, 0.5]
        ],

        /// Negative reward?
        "reward_sign": false,

        "policy_optimizer": "adam",
        "policy_learning_rate": 0.02,
        "policy_learning_rate_discount": 0.7,
        "policy_learning_rate_discount_freq": 50,

        /// MLP policy parameters
        "hidden_size": 12,

        "gamma": 0.9,
        "b_init": 2.0,
        // Use normalize when initializing W
        "W_normalize": true,

        "reward_baseline": false,
        "reward_baseline_update_rate": 0.8,

        /// Actor-Critic configurations
        "actor_gamma": 1.0,

        // Use cost gap before and after update as immediate reward
        // Another immediate reward is the accuracy of sampled validation set
        "cost_gap_AC_reward": false,

        // Update actor and critic frequency
        "AC_update_freq": 100,

        /// Self-paced learning configurations
        "epoch_update": false,

        "start_cost": 0.6,
        "end_cost": 4.60517     // -log(0.01)
    },

    "cifar10": {
        "data_dir": "./data/cifar-10-batches-py",

        /// Model name.
        // Candidates:
        //     CIFARModel
        //     VaniliaCNNModel
        "model_name": "CIFARModel",

        "model_file": "./data/cifar10/cifar10_deep_residual_model.npz",
        "save_model": false,

        "total_size": 60000,
        "train_size": 50000,
        "validation_size": 10000,
        "test_size": 10000,

        "curriculum": false,
        "random_drop": false,
        "random_drop_number_file": "./data/cifar10/drop_num.txt",

        "use_policy": true,
        "train_small_size": 40000,
        "epoch_per_episode": 82,

        "warm_start": false,

        "init_learning_rate": 0.1,
        "learning_rate_discount": 0.1,

        "l2_penalty_factor": 0.0001,
        "momentum": 0.9,

        "train_batch_size": 128,
        "validate_batch_size": 500,
        "cnn_output_size": 10,
        "n": 5,
        "num_epochs": 5000,

        "print_label_distribution": true,

        "display_freq": 20
    },

    "imdb": {
        "seed": 123,

        "random_drop_number_file": "./data/imdb/drop_num.txt",

        "dataset": "imdb",
        "data_dir": "./data/imdb/imdb.pkl",

        "train_small_size": 10000,

        "test_size": 5000,
        "valid_portion": 0.05,
        "n_words": 10000,
        "maxlen": null,

        "epoch_per_episode": 100,

        "dim_proj": 128,
        "patience": 10,
        "display_freq": 10,

        "decay_c": 0.0,
        "learning_rate": 0.0001,

        "save_to": false, //"./data/imdb/lstm_model.npz",
        "valid_freq": 370,
        "save_freq": 1110,
        "train_loss_freq": 370,

        "train_batch_size": 16,
        "validate_batch_size": 64,

        "optimizer": "adadelta",

        "noise_std": 0.0,
        "reload_model": false,
        "use_dropout": true
    },

    "mnist": {
        "seed": 1234,

        "dataset": "mnist",
        "data_dir": "./data/mnist/mnist.pkl.gz",

        "random_drop_number_file": "./data/mnist/drop_num.txt",

        /// Valid per ?? batches (20 * ?? data)
        "valid_freq": 250,
        "train_loss_freq": 0,
        "display_freq": 100,

        "train_small_size": 50000,

        "epoch_per_episode": 400,

        "hidden_size": 500,
        "learning_rate": 0.01,
        "train_batch_size": 20,
        "validate_batch_size": 20,

        "l1_penalty_factor": 0.00,
        "l2_penalty_factor": 0.0001,

        "patience": 10000,
        "patience_increase": 2,
        "improvement_threshold": 0.995,

        // 0 means do not test at validate point
        "test_per_point": 1,

        // When train raw, get shuffled data.
        "raw_shuffle": true,

        "warm_start": false,
        "warm_start_model_file": "./data/mnist/warm_start_model.npz",

        "save_model": false,
        "save_model_file": "./data/mnist/warm_start_model.npz"
    }
}
